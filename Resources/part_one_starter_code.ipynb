{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"id":"-8eFW_wl1n39","outputId":"0e9cc6e3-4928-4276-a9b0-9e15789de0cb","executionInfo":{"status":"error","timestamp":1677956303968,"user_tz":300,"elapsed":66945,"user":{"displayName":"Joan Adebowale","userId":"03943659896594566167"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=3c3ba28d08658f5076f83c3a5d8bb35a730bf2996aa2218c624ed2f026f4c0a3\n","  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n","Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n","Hit:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n","Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n","Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Hit:10 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Fetched 108 kB in 2s (46.7 kB/s)\n","Reading package lists... Done\n"]},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8d3284935bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Start a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.2.2-bin-hadoop2.7/python, your SPARK_HOME may not be configured correctly"]}],"source":["import os\n","!pip install pyspark\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","!pip install -q findspark\n","# Activate Spark in our Colab notebook.\n","# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.2.2'\n","spark_version = 'spark-3.2.2'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","\n","# Set Environment Variables\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzCrgs0Z1rnw","outputId":"9c7492fe-15f2-4fe5-e818-ed651cd61a8e","executionInfo":{"status":"ok","timestamp":1677956322831,"user_tz":300,"elapsed":686,"user":{"displayName":"Joan Adebowale","userId":"03943659896594566167"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-03-04 18:58:42--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n","Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n","Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 914037 (893K) [application/java-archive]\n","Saving to: ‘postgresql-42.2.9.jar.2’\n","\n","postgresql-42.2.9.j 100%[===================>] 892.61K  5.63MB/s    in 0.2s    \n","\n","2023-03-04 18:58:42 (5.63 MB/s) - ‘postgresql-42.2.9.jar.2’ saved [914037/914037]\n","\n"]}],"source":["# Get postgresql package\n","!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0DuBth0V2PR8","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"error","timestamp":1677956324630,"user_tz":300,"elapsed":292,"user":{"displayName":"Joan Adebowale","userId":"03943659896594566167"}},"outputId":"d999f7bb-8c99-4986-f0f7-947061246f25"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ec5d6cb0d628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Spark and create a SparkSessionfrom pyspark.sql import SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BigData-HW-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.extraClassPath\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/postgresql-42.2.9.jar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preexec_fn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    856\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-3.2.2-bin-hadoop2.7/./bin/spark-submit'"]}],"source":["# Import Spark and create a SparkSessionfrom pyspark.sql import SparkSession\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"BigData-HW-1\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"D3W2XJVi2CU-"},"source":["# Extract the Amazon Data into Spark DataFrame"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Na_stw7b1wfU","executionInfo":{"status":"ok","timestamp":1677956389211,"user_tz":300,"elapsed":146,"user":{"displayName":"Joan Adebowale","userId":"03943659896594566167"}}},"outputs":[],"source":["# Read in the data from an S3 Bucket\n","from pyspark import SparkFiles\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cayz-3Q52IM3","outputId":"5d19aeec-223a-4372-f773-f74ab1aabc62"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1785997"]},"metadata":{},"execution_count":5}],"source":["# Get the number of rows in the DataFrame.\n"]},{"cell_type":"markdown","metadata":{"id":"C9U0rkGZ2eu7"},"source":["# Transform the Data"]},{"cell_type":"markdown","source":["## Create the \"review_id_table\"."],"metadata":{"id":"dUoftWoKtM_c"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tMYkSIk2d-m","outputId":"e090226f-d7f3-4319-c47f-8a4fc9a69c09"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+-----------+----------+--------------+-----------+\n","|     review_id|customer_id|product_id|product_parent|review_date|\n","+--------------+-----------+----------+--------------+-----------+\n","| RTIS3L2M1F5SM|   12039526|B001CXYMFS|     737716809| 2015-08-31|\n","| R1ZV7R40OLHKD|    9636577|B00M920ND6|     569686175| 2015-08-31|\n","|R3BH071QLH8QMC|    2331478|B0029CSOD2|      98937668| 2015-08-31|\n","|R127K9NTSXA2YH|   52495923|B00GOOSV98|      23143350| 2015-08-31|\n","|R32ZWUXDJPW27Q|   14533949|B00Y074JOM|     821342511| 2015-08-31|\n","|R3AQQ4YUKJWBA6|    2377552|B002UBI6W6|     328764615| 2015-08-31|\n","|R2F0POU5K6F73F|   17521011|B008XHCLFO|      24234603| 2015-08-31|\n","|R3VNR804HYSMR6|   19676307|B00BRA9R6A|     682267517| 2015-08-31|\n","| R3GZTM72WA2QH|     224068|B009EPWJLA|     435241890| 2015-08-31|\n","| RNQOY62705W1K|   48467989|B0000AV7GB|     256572651| 2015-08-31|\n","|R1VTIA3JTYBY02|     106569|B00008KTNN|     384411423| 2015-08-31|\n","|R29DOU8791QZL8|   48269642|B000A3IA0Y|     472622859| 2015-08-31|\n","|R15DUT1VIJ9RJZ|   52738710|B0053BQN34|     577628462| 2015-08-31|\n","|R3IMF2MQ3OU9ZM|   10556786|B002I0HIMI|     988218515| 2015-08-31|\n","|R23H79DHOZTYAU|    2963837|B0081EH12M|     770100932| 2015-08-31|\n","| RIV24EQAIXA4O|   23092109|B005FMLZQQ|      24647669| 2015-08-31|\n","|R3UCNGYDVN24YB|   23091728|B002BSA388|      33706205| 2015-08-31|\n","| RUL4H4XTTN2DY|   10712640|B00BUSLSAC|     829667834| 2015-08-31|\n","|R20JF7Z4DHTNX5|   17455376|B00KWF38AW|     110680188| 2015-08-31|\n","|R2T1AJ5MFI2260|   14754850|B00BRQJYA8|     616463426| 2015-08-31|\n","+--------------+-----------+----------+--------------+-----------+\n","only showing top 20 rows\n","\n"]}],"source":["from pyspark.sql.functions import to_date\n","# Create the \"review_id_df\" DataFrame with the appropriate columns and data types.\n"]},{"cell_type":"markdown","source":["## Create the \"products\" Table"],"metadata":{"id":"aAVCFjXhtXO8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9gTNhT62je4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c6e97a7-c616-4229-e2f3-29e043b53833"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+--------------------+\n","|product_id|       product_title|\n","+----------+--------------------+\n","|B00CJ7IUI6|The Elder Scrolls...|\n","|B00DHF39KS|Wolfenstein: The ...|\n","|B00MUTAVH6|Under Night In-Bi...|\n","|B001AZSEUW|              Peggle|\n","|B00KVOVBGM|PlayStation 4 Con...|\n","|B00O9VGH4Y|USPRO&reg; Headph...|\n","|B004OQNZY4|Phineas and Ferb:...|\n","|B00ZLN980O|Donop seablue 2.4...|\n","|B002L8W5V6|Dotop Nintendo Ga...|\n","|B007AJZ5PY|Nyko Game Case fo...|\n","|B000AOEU2K|Fire Emblem: Path...|\n","|B000H8BW7U|Tanarus (PC) (Com...|\n","|B013RADQOQ|Susenstone® 2400D...|\n","|B00KQXKUJ2|FIFA 15 (Ultimate...|\n","|B006W41X2C|Turtle Beach - Ea...|\n","|B000KCX9M4|Grand Theft Auto:...|\n","|B00YT90JWC|Red Wii Mini Cons...|\n","|B0096KG6A8|Wii U Super Mario...|\n","|B00L6AVLB0|World of Tanks-X3...|\n","|B000IMYKQ0|Wii Nunchuk Contr...|\n","+----------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the \"products_df\" DataFrame that drops the duplicates in the \"product_id\" and \"product_title columns. \n"]},{"cell_type":"markdown","source":["## Create the \"customers\" Table"],"metadata":{"id":"LJHuZ9zut0e5"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pF2Vf3c2n2O","outputId":"9214d06e-83e1-40cf-d209-e1a571ba03cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+--------------+\n","|customer_id|customer_count|\n","+-----------+--------------+\n","|   48670265|             1|\n","|   49103216|             2|\n","|    1131200|             1|\n","|   43076447|             2|\n","|   46261368|             1|\n","|    4883305|             5|\n","|   41192649|             1|\n","|   40985731|             7|\n","|   10437900|             2|\n","|   22245671|             1|\n","|    2574873|             1|\n","|    4696154|             1|\n","|    5621202|             1|\n","|    5871933|             2|\n","|   44089812|             1|\n","|    2845910|             1|\n","|    5274369|             1|\n","|   39069693|             2|\n","|     137793|             1|\n","|   31914942|             3|\n","+-----------+--------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Create the \"customers_df\" DataFrame that groups the data on the \"customer_id\" by the number of times a customer reviewed a product. \n"]},{"cell_type":"markdown","source":["## Create the \"vine_table\"."],"metadata":{"id":"8SbTasxbuXGK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHQKbmCE2p3Q","outputId":"5226601d-26b3-4853-f69f-3fc9bec82369"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+-----------+-------------+-----------+----+\n","|     review_id|star_rating|helpful_votes|total_votes|vine|\n","+--------------+-----------+-------------+-----------+----+\n","| RTIS3L2M1F5SM|          5|            0|          0|   N|\n","| R1ZV7R40OLHKD|          5|            0|          0|   N|\n","|R3BH071QLH8QMC|          1|            0|          1|   N|\n","|R127K9NTSXA2YH|          3|            0|          0|   N|\n","|R32ZWUXDJPW27Q|          4|            0|          0|   N|\n","|R3AQQ4YUKJWBA6|          1|            0|          0|   N|\n","|R2F0POU5K6F73F|          5|            0|          0|   N|\n","|R3VNR804HYSMR6|          5|            0|          0|   N|\n","| R3GZTM72WA2QH|          5|            0|          0|   N|\n","| RNQOY62705W1K|          4|            0|          0|   N|\n","+--------------+-----------+-------------+-----------+----+\n","only showing top 10 rows\n","\n"]}],"source":["# Create the \"vine_df\" DataFrame that has the \"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", and \"vine\" columns. \n"]},{"cell_type":"markdown","metadata":{"id":"I8aTsEjZ2s6L"},"source":["# Load"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"W4dzUKfI2vXM","executionInfo":{"status":"ok","timestamp":1677956496320,"user_tz":300,"elapsed":139,"user":{"displayName":"Joan Adebowale","userId":"03943659896594566167"}}},"outputs":[],"source":["mode = \"append\"\n","jdbc_url=\"jdbc:postgresql://<endpoint>:5432/my_data_class_db\"\n","config = {\"user\":\"postgres\", \"password\": \"<password>\", \"driver\":\"org.postgresql.Driver\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOxKqMsD2yVs"},"outputs":[],"source":["# Write review_id_df to table in RDS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPXyGVE-2yPJ"},"outputs":[],"source":["# Write products_df to table in RDS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHbca4zN2yIa"},"outputs":[],"source":["# Write customers_df to table in RDS\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HfOFneW2x_F"},"outputs":[],"source":["# Write vine_df to table in RDS\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}